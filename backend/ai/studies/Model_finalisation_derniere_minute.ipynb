{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle 1 : Finalisation dernière minute\n",
    "\n",
    "- On ajoute des poids pour bénéficier \"Définition\" \\*1.3 et réduire \"Titre\" \\*0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas spacy bs4 sentence_transformers numpy\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrice/Documents/Etudes/CY-Tech/Ing2_Sem2_2023-2024/UE7_Projet_DataBattle_IA-Pau/ia-pau-data-battle-mars-2024/backend/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouvelles fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On traite les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(csv_file='../data/solutions2.csv'):\n",
    "    dictionnaire_solution = {1: \"Titre\", 2: \"Description\", 5: \"Application\", 6: \"Bilan énergétique\", 21: \"Titre technologie\", 22: \"Description technologie\"}\n",
    "\n",
    "    # Charger le fichier CSV en spécifiant le séparateur '|'\n",
    "    df = pd.read_csv(csv_file, sep='|', header=None)\n",
    "\n",
    "    # Initialiser une liste pour stocker les données de chaque solution\n",
    "    solutions_data = []\n",
    "\n",
    "    # Parcourir chaque ligne du DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        id_sol = row[0]\n",
    "        section = row[1]\n",
    "        texte = row[2]\n",
    "\n",
    "        # Vérifier si la section correspond à une clé dans le dictionnaire de solutions\n",
    "        if section in dictionnaire_solution:\n",
    "            # Récupérer le nom de la section\n",
    "            section_name = dictionnaire_solution[section]\n",
    "\n",
    "            # Chercher si la solution existe déjà dans la liste\n",
    "            solution_exists = False\n",
    "            for solution in solutions_data:\n",
    "                if solution[0] == id_sol:\n",
    "                    solution_exists = True\n",
    "                    solution[1][section_name] = texte\n",
    "                    break\n",
    "\n",
    "            # Si la solution n'existe pas encore, la créer\n",
    "            if not solution_exists:\n",
    "                new_solution = [id_sol, {section_name: texte}]\n",
    "                solutions_data.append(new_solution)\n",
    "\n",
    "    return solutions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction pour obtenir les données\n",
    "# df_solutions = load_and_merge_data()\n",
    "# print(df_solutions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de langue SpaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def clean_df_solutions(df_solutions):\n",
    "    cleaned_data = []\n",
    "    for item in df_solutions:\n",
    "        index = item[0]\n",
    "        solution = item[1]\n",
    "        cleaned_solution = {}\n",
    "\n",
    "        for key, value in solution.items():\n",
    "            if (key == 'Titre' or key == 'Titre technologie') :\n",
    "                # Pour les titres, ne pas enlever les chiffres\n",
    "                cleaned_solution[key] = clean_text(str(value), remove_numbers=False)\n",
    "            else:\n",
    "                # Pour les autres champs, enlever les chiffres\n",
    "                cleaned_solution[key] = clean_text(str(value), remove_numbers=True)\n",
    "\n",
    "        cleaned_data.append([index, cleaned_solution])\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def clean_text(text, remove_numbers=True):\n",
    "    # Nettoyer HTML Tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # Remplacer \"&nbsp;.\" par rien\n",
    "    text = re.sub(r'&nbsp;\\.', '', text)\n",
    "\n",
    "    # Supprimer les \"l'\"\n",
    "    text = re.sub(r\"\\bl'\", '', text)\n",
    "\n",
    "    # Accents\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "\n",
    "    if remove_numbers:\n",
    "        # Retirer les numéros\n",
    "        text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "    # Supprimer les caractères seuls\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "    # Tokenization, Lemmatization, Removing Stopwords, Lowercase\n",
    "    doc = nlp(text)\n",
    "    cleaned_sentences = []\n",
    "    for sentence in doc.sents:\n",
    "        tokens = [token.lemma_.lower() for token in sentence if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "        clean_sentence = ' '.join(tokens)\n",
    "        if clean_sentence:\n",
    "            cleaned_sentences.append(clean_sentence)  # Ajouter un point à la fin de la phrase propre\n",
    "\n",
    "    # Joining the cleaned sentences back into a single string\n",
    "    cleaned_text = ' '.join(cleaned_sentences)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_solutions_clean = clean_df_solutions(df_solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_solutions_clean[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier qu'on garde les numéros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_solution_by_id(df_solutions_clean, solution_id):\n",
    "#     for item in df_solutions_clean:\n",
    "#         if item[0] == solution_id:\n",
    "#             return item[1]\n",
    "#     return None\n",
    "\n",
    "# # Utilisation de la fonction pour obtenir le dictionnaire de l'ID 22\n",
    "# solution_id_choisi = 1692\n",
    "# solution_choisie = get_solution_by_id(df_solutions_clean, solution_id_choisi)\n",
    "\n",
    "# # Afficher le dictionnaire de la solution choisie\n",
    "# print(solution_choisie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On charge le modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des embeddings\n",
    "\n",
    "On va sauvegarder les embeddings en gardant le nom des champs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_embeddings(data, model, output_file):\n",
    "    # Faire une copie de data\n",
    "    data = copy.deepcopy(data)\n",
    "    # Fonction pour encoder chaque texte\n",
    "    def encoder_texte(texte):\n",
    "        return model.encode(texte)\n",
    "    # Pour chaque entrée dans les données, encoder tous les champs texte\n",
    "    for entry in data:\n",
    "        # print(\"DEBUG : entry = \", entry)\n",
    "        for champ, valeur in entry[1].items():\n",
    "            # print(\"DEBUG : champ =\", champ, \", valeur = \", valeur)\n",
    "            if isinstance(valeur, str):  # S'assurer que la valeur est une chaîne de caractères\n",
    "                # Calculer l'embedding\n",
    "                embedding = encoder_texte(valeur)\n",
    "                # Remplacer le texte par l'embedding\n",
    "                entry[1][champ] = embedding.tolist()\n",
    "    # Sauvegarder les embeddings dans un fichier\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_embedding(text, model):\n",
    "    # Diviser le texte en phrases\n",
    "    sentences = [sentence.strip() for sentence in text.split('.') if sentence.strip()]\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    # Prendre la moyenne des embeddings des phrases\n",
    "    if len(sentence_embeddings) > 0:\n",
    "        average_embedding = np.mean(sentence_embeddings, axis=0)\n",
    "    else:\n",
    "        average_embedding = np.zeros(model.get_sentence_embedding_dimension())\n",
    "    return average_embedding\n",
    "\n",
    "def encoder_embeddings_moyenne_sentences(data, model, output_file):\n",
    "    # Faire une copie de data\n",
    "    data = copy.deepcopy(data)\n",
    "    # Pour chaque entrée dans les données, encoder tous les champs texte\n",
    "    for entry in data:\n",
    "        # print(\"DEBUG : entry = \", entry)\n",
    "        for champ, valeur in entry[1].items():\n",
    "            # print(\"DEBUG : champ =\", champ, \", valeur = \", valeur)\n",
    "            if isinstance(valeur, str):  # S'assurer que la valeur est une chaîne de caractères\n",
    "                # Calculer l'embedding en faisant la moyenne de l'embedding de chaque phrase.\n",
    "                embedding = calculate_average_embedding(valeur, model)\n",
    "                # Remplacer le texte par l'embedding\n",
    "                entry[1][champ] = embedding.tolist()\n",
    "    # Sauvegarder les embeddings dans un fichier\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par calculer un embedding sur deux solutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_magb_sans_somme_sentences.pkl\"\n",
    "# solutions_embeddings = encoder_embeddings(df_solutions_clean, model, path_embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_moyenne_sentences_magb.pkl\"\n",
    "# solutions_embeddings = encoder_embeddings_moyenne_sentences(df_solutions_clean, model, path_embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pour voir notre embedding sur notre première solution.\n",
    "# for champ, valeur in solutions_embeddings[0][1].items():\n",
    "#     print(champ,\" : \", valeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(solutions_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction appelé par notre utilisateur\n",
    "def model_find_solution(description, secteur, model, embeddings_file_path, seuil_similarite, min_sol, weights) :\n",
    "    # !!!!!!!!!!!! ICI ON A CHOISI DE NE PAS INCLURE LE SECTEUR DANS NOTRE MODELE, COMMENTER LA LIGNE CI-DESOSUS POUR CHANGER CELA --------------\n",
    "    secteur = \"\"\n",
    "    # -------------------------\n",
    "\n",
    "    # On commence par concaténer notre secteur et notre description.\n",
    "    requete = secteur + \". \" + description\n",
    "\n",
    "    # Ensuite on applique notre pré-processing\n",
    "    clean_requete = clean_text(requete, remove_numbers=False)\n",
    "    clean_requete_vecteur =  model.encode(clean_requete)\n",
    "\n",
    "    # On va lire notre fichier d'embeddings \n",
    "    with open(embeddings_file_path, \"rb\") as fIn:\n",
    "        solutions_embeddings = pickle.load(fIn)\n",
    "\n",
    "\n",
    "    solutions_similarities = []\n",
    "    # Maintenant pour chaque solution on va garder notre meilleur similarité cosinus avec notre requete. De plus nous ajouton un poids à chaque champs de notre solution,\n",
    "    # Description * 1.3, Titre * 0.7, et 1 pour tous les autres.\n",
    "    # On se retrouve donc avec une liste [[id_sol, max_similarité], ...]\n",
    "\n",
    "    # Pour chaque entrée dans les données, encoder tous les champs texte\n",
    "    for entry in solutions_embeddings:\n",
    "        max_similarity = 0\n",
    "        id_solution = entry[0]\n",
    "        for champ, valeur_vecteur in entry[1].items():\n",
    "            # Coeficient multiplicateur \n",
    "            weight = weights[champ]\n",
    "            # On calcul la similarité\n",
    "            similarity = util.pytorch_cos_sim(valeur_vecteur, clean_requete_vecteur)*weight\n",
    "            # On met à jour le max de similarité\n",
    "            if similarity > max_similarity :\n",
    "                max_similarity = similarity\n",
    "        solution_similarity = [id_solution, max_similarity]\n",
    "        solutions_similarities.append(solution_similarity)\n",
    "    #print(\"DEBUG : \",solutions_similarities)\n",
    "    \n",
    "    \n",
    "    # On trie notre liste de solution par ordre croissant\n",
    "    solutions_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # On sélectionne les 5 meilleures solutions\n",
    "    top_5_solutions = [sol[0] for sol in solutions_similarities[:min_sol]]\n",
    "\n",
    "    # On applique le seuil de similarité aux solutions restantes\n",
    "    remaining_solutions = [sol[0] for sol in solutions_similarities[min_sol:] if sol[1] > seuil_similarite]\n",
    "\n",
    "    # On concatène les deux listes de solutions\n",
    "    final_solutions = top_5_solutions + remaining_solutions\n",
    "\n",
    "    # On retourne que les solutions et non la similarité\n",
    "    return final_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\"Titre\":1, \"Description\":1, \"Application\":1, \"Bilan énergétique\":1, \"Titre technologie\":1, \"Description technologie\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_magb_avant_ajout_points.pkl\"\n",
    "# model_find_solution(\"C'est quoi la HP flottante ?\", \"\", model, path_embedding_file, 0.7, 5, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_magb.pkl\"\n",
    "# model_find_solution(\"C'est quoi la HP flottante ?\", \"\", model, path_embedding_file, 0.7, 5, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_moyenne_sentences_magb.pkl\"\n",
    "# model_find_solution(\"C'est quoi la HP flottante ?\", \"\", model, path_embedding_file, 0.7, 5, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[724, 835, 1533, 1608, 1595]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_embedding_file = \"embeddings/model_final_magb_sans_somme_sentences.pkl\"\n",
    "model_find_solution(\"C'est quoi la HP flottante ?\", \"\", model, path_embedding_file, 0.7, 5, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester sur dataset Kerdos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire le fichier CSV\n",
    "df_testset = pd.read_csv(\"data/dataset_test_Kerdos.csv\")\n",
    "#df_testset = pd.read_csv(\"data/patrice_test_set.csv\")\n",
    "\n",
    "\n",
    "def test_accuracy(path_embedding_file, dataset=df_testset, top_n=1) :\n",
    "    accuracy = 0\n",
    "    for i in range(1,len(dataset)):\n",
    "        predictions = model_find_solution(dataset['Description'][i], \"\", model, path_embedding_file, 0.8, 5, weights)\n",
    "        # print(\"---------------------------------------\")\n",
    "        # print(\"Valeur attendu : \", dataset[\"id_solution\"][i])\n",
    "        # print(predictions)\n",
    "        if (dataset[\"id_solution\"][i] in predictions[:top_n]):\n",
    "            accuracy += 1/len(dataset)\n",
    "        # else :\n",
    "        #     print(\"mal prédit : \", dataset[\"id_solution\"][i])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7600000000000003"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_magb_avant_ajout_points.pkl\"\n",
    "# test_accuracy(path_embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8000000000000004"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_magb.pkl\"\n",
    "# test_accuracy(path_embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7600000000000003"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_embedding_file = \"embeddings/model_final_moyenne_sentences_magb.pkl\"\n",
    "# test_accuracy(path_embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_embedding_file = \"embeddings/model_final_magb_sans_somme_sentences.pkl\"\n",
    "test_accuracy(path_embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FONCTIONS POUR OPTIMISER ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répétition des fonctions à run exclusivement pour ces tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset et weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire le fichier CSV\n",
    "df_testset = pd.read_csv(\"data/dataset_test_Kerdos.csv\")\n",
    "#df_testset = pd.read_csv(\"data/patrice_test_set.csv\")\n",
    "\n",
    "weights = {\"Titre\":1, \"Description\":1, \"Application\":1, \"Bilan énergétique\":1, \"Titre technologie\":1, \"Description technologie\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.4375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(weights_to_try\u001b[38;5;241m.\u001b[39mkeys(), weights_combination))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculer l'accuracy pour cette combinaison de poids\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_embedding_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_testset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Vérifier si c'est la meilleure accuracy jusqu'à présent\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mtest_accuracy\u001b[0;34m(path_embedding_file, dataset, top_n)\u001b[0m\n\u001b[1;32m      7\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m----> 9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_find_solution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_embedding_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# print(\"---------------------------------------\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# print(\"Valeur attendu : \", dataset[\"id_solution\"][i])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print(predictions)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;129;01min\u001b[39;00m predictions[:top_n]):\n",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m, in \u001b[0;36mmodel_find_solution\u001b[0;34m(description, secteur, model, embeddings_file_path, seuil_similarite, min_sol, weights)\u001b[0m\n\u001b[1;32m     30\u001b[0m weight \u001b[38;5;241m=\u001b[39m weights[champ]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# On calcul la similarité\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch_cos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvaleur_vecteur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_requete_vecteur\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mweight\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# On met à jour le max de similarité\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m max_similarity :\n",
      "File \u001b[0;32m~/Documents/Etudes/CY-Tech/Ing2_Sem2_2023-2024/UE7_Projet_DataBattle_IA-Pau/ia-pau-data-battle-mars-2024/backend/venv/lib/python3.11/site-packages/sentence_transformers/util.py:28\u001b[0m, in \u001b[0;36mpytorch_cos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpytorch_cos_sim\u001b[39m(a: Tensor, b: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Etudes/CY-Tech/Ing2_Sem2_2023-2024/UE7_Projet_DataBattle_IA-Pau/ia-pau-data-battle-mars-2024/backend/venv/lib/python3.11/site-packages/sentence_transformers/util.py:38\u001b[0m, in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mComputes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m:return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 38\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     41\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importations nécessaires\n",
    "import itertools\n",
    "\n",
    "# Définition des poids à tester\n",
    "weights_to_try = {\n",
    "    \"Titre\": [0.5, 1],\n",
    "    \"Description\": [0.5, 1],\n",
    "    \"Application\": [0.5, 1],\n",
    "    \"Bilan énergétique\": [0.5, 1],\n",
    "    \"Titre technologie\": [0.5, 1],\n",
    "    \"Description technologie\": [0.5, 1],\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_weights = None\n",
    "\n",
    "# Parcourir toutes les combinaisons de poids\n",
    "for weights_combination in itertools.product(*weights_to_try.values()):\n",
    "    # Créer un dictionnaire de poids à partir de la combinaison\n",
    "    weights = dict(zip(weights_to_try.keys(), weights_combination))\n",
    "    \n",
    "    # Calculer l'accuracy pour cette combinaison de poids\n",
    "    accuracy = test_accuracy(path_embedding_file, df_testset, top_n=3)\n",
    "    \n",
    "    # Vérifier si c'est la meilleure accuracy jusqu'à présent\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        print(\"Best accuracy : \", best_accuracy)\n",
    "        best_weights = weights.copy()\n",
    "\n",
    "print(\"Meilleure accuracy trouvée:\", best_accuracy)\n",
    "print(\"Meilleurs poids associés:\")\n",
    "print(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.1875\n",
      "Best weight :  {'Titre': 0.7000000000000001, 'Description': 1.3000000000000003, 'Application': 1.4000000000000001, 'Bilan énergétique': 0.9, 'Titre technologie': 1.0, 'Description technologie': 0.7000000000000001}\n",
      "Best accuracy :  0.4375\n",
      "Best weight :  {'Titre': 0.7000000000000001, 'Description': 0.6, 'Application': 0.5, 'Bilan énergétique': 0.30000000000000004, 'Titre technologie': 0.2, 'Description technologie': 0.6}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m weights \u001b[38;5;241m=\u001b[39m {key: random\u001b[38;5;241m.\u001b[39mchoice(values) \u001b[38;5;28;01mfor\u001b[39;00m key, values \u001b[38;5;129;01min\u001b[39;00m weights_to_try\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculer l'accuracy pour cette combinaison de poids\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_embedding_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_testset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Vérifier si c'est la meilleure accuracy jusqu'à présent\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mtest_accuracy\u001b[0;34m(path_embedding_file, dataset, top_n)\u001b[0m\n\u001b[1;32m      7\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m----> 9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_find_solution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_embedding_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# print(\"---------------------------------------\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# print(\"Valeur attendu : \", dataset[\"id_solution\"][i])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print(predictions)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;129;01min\u001b[39;00m predictions[:top_n]):\n",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m, in \u001b[0;36mmodel_find_solution\u001b[0;34m(description, secteur, model, embeddings_file_path, seuil_similarite, min_sol, weights)\u001b[0m\n\u001b[1;32m     30\u001b[0m weight \u001b[38;5;241m=\u001b[39m weights[champ]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# On calcul la similarité\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch_cos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvaleur_vecteur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_requete_vecteur\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mweight\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# On met à jour le max de similarité\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m max_similarity :\n",
      "File \u001b[0;32m~/Documents/Etudes/CY-Tech/Ing2_Sem2_2023-2024/UE7_Projet_DataBattle_IA-Pau/ia-pau-data-battle-mars-2024/backend/venv/lib/python3.11/site-packages/sentence_transformers/util.py:28\u001b[0m, in \u001b[0;36mpytorch_cos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpytorch_cos_sim\u001b[39m(a: Tensor, b: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Etudes/CY-Tech/Ing2_Sem2_2023-2024/UE7_Projet_DataBattle_IA-Pau/ia-pau-data-battle-mars-2024/backend/venv/lib/python3.11/site-packages/sentence_transformers/util.py:38\u001b[0m, in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mComputes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m:return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 38\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     41\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Définition des poids à tester\n",
    "weights_to_try = {\n",
    "    \"Titre\": np.arange(0.1, 1.6, 0.1).tolist(),\n",
    "    \"Description\": np.arange(0.1, 1.6, 0.1).tolist(),\n",
    "    \"Application\": np.arange(0.1, 1.6, 0.1).tolist(),\n",
    "    \"Bilan énergétique\": np.arange(0.1, 1.6, 0.1).tolist(),\n",
    "    \"Titre technologie\": np.arange(0.1, 1.6, 0.1).tolist(),\n",
    "    \"Description technologie\": np.arange(0.1, 1.6, 0.1).tolist(),\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_weights = None\n",
    "num_iterations = 100  # Nombre d'itérations pour la recherche aléatoire\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Générer une combinaison aléatoire de poids\n",
    "    weights = {key: random.choice(values) for key, values in weights_to_try.items()}\n",
    "    \n",
    "    # Calculer l'accuracy pour cette combinaison de poids\n",
    "    accuracy = test_accuracy(path_embedding_file, df_testset, top_n=3)\n",
    "    \n",
    "    # Vérifier si c'est la meilleure accuracy jusqu'à présent\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        print(\"Best accuracy : \", best_accuracy)\n",
    "        best_weights = weights.copy()\n",
    "        print(\"Best weight : \", best_weights)\n",
    "\n",
    "print(\"Meilleure accuracy trouvée:\", best_accuracy)\n",
    "print(\"Meilleurs poids associés:\")\n",
    "print(best_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
